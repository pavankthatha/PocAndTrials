{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Lexicons or Word Sense Disambiguation ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') a long ridge or pile\n",
      "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') a building in which the business of banking transacted\n",
      "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') tip laterally\n",
      "Synset('bank.v.02') enclose with a bank\n",
      "Synset('bank.v.03') do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') be in the banking business\n",
      "Synset('deposit.v.02') put into a bank account\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bank'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n"
     ]
    }
   ],
   "source": [
    "sent = ['I', 'went', 'to', 'bank', 'of', 'river', ',','cold', 'breeze', 'was','flowing','.']\n",
    "print(lesk(sent, 'bank', 'n').definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a large natural stream of water (larger than a creek)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('river.n.01').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypernym\n",
    "** A hypernym is a word that names a broad category that includes other words. \"Primate\" is a hypernym for \"chimpanzee\" and \"human.\" Whenever you see a word ending in nym, you know it's going to be some kind of name. In this case, a hypernym is the name of a broader category of things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United States educator who was born a slave but became educated and founded a college at Tuskegee in Alabama (1856-1915)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesk(sent, 'bank', 'n').tree(lambda s:s.hypernyms())\n",
    "lesk(\"I went to Washington, I liked the place\", 'Washington', 'n').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#nouns\n",
    "#token_pos_tagged[990:1100]\n",
    "sentence = \"Subhash Patil the great. He talked to a person, \\\n",
    "                                                 it's the greatest act from him \\\n",
    "                                                \"\n",
    "sentenceTagged = nltk.pos_tag(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#sentenceTagged.sort()\\ngroucho_grammar = nltk.CFG.fromstring(\"\"\"\\nS -> NP VP\\nPP -> P NP\\nNP -> Det N | Det N PP | \\'I\\'\\nVP -> V NP | VP PP\\n\"\"\")\\nsentenceTagged\\nparser = nltk.ChartParser(groucho_grammar)\\nfor tree in parser.parse(sentence):\\n    print(tree)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#sentenceTagged.sort()\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "\"\"\")\n",
    "sentenceTagged\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using stanform NER\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('/home/tivo/tivo/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "'/home/tivo/tivo/stanford-ner/stanford-ner.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Rami', 'PERSON'),\n",
       " ('Eid', 'PERSON'),\n",
       " ('is', 'O'),\n",
       " ('studying', 'O'),\n",
       " ('at', 'O'),\n",
       " ('Stony', 'ORGANIZATION'),\n",
       " ('Brook', 'ORGANIZATION'),\n",
       " ('University', 'ORGANIZATION'),\n",
       " ('in', 'O'),\n",
       " ('NY', 'O')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.tag('Rami Eid is studying at Stony Brook University in NY'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Copied from NLTK documentation\\nADJ --> adjective \\tnew, good, high, special, big, local\\nADP \\tadposition \\ton, of, at, with, by, into, under\\nADV \\tadverb \\treally, already, still, early, now\\nCONJ \\tconjunction \\tand, or, but, if, while, although\\nDET \\tdeterminer, article \\tthe, a, some, most, every, no, which\\nNOUN \\tnoun \\tyear, home, costs, time, Africa\\nNUM \\tnumeral \\ttwenty-four, fourth, 1991, 14:24\\nPRT \\tparticle \\tat, on, out, over per, that, up, with\\nPRON \\tpronoun \\the, their, her, its, my, I, us\\nVERB \\tverb \\tis, say, told, given, playing, would\\n. \\tpunctuation marks \\t. , ; !\\nX \\tother \\tersatz, esprit, dunno, gr8, univeristy\\n}\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TagVsWordNetMap = {\n",
    "    'ADJ': 'a',\n",
    "    'ADJ_SAT': 's',\n",
    "    'ADV': 'r',\n",
    "    'NOUN': 'n',\n",
    "    'NNP' : 'n',\n",
    "    'VERB': 'v',\n",
    "    'VBN': 'v',\n",
    "    'VBD':'v'\n",
    "    }\n",
    "'''\n",
    "# Copied from NLTK documentation\n",
    "ADJ --> adjective \tnew, good, high, special, big, local\n",
    "ADP \tadposition \ton, of, at, with, by, into, under\n",
    "ADV \tadverb \treally, already, still, early, now\n",
    "CONJ \tconjunction \tand, or, but, if, while, although\n",
    "DET \tdeterminer, article \tthe, a, some, most, every, no, which\n",
    "NOUN \tnoun \tyear, home, costs, time, Africa\n",
    "NUM \tnumeral \ttwenty-four, fourth, 1991, 14:24\n",
    "PRT \tparticle \tat, on, out, over per, that, up, with\n",
    "PRON \tpronoun \the, their, her, its, my, I, us\n",
    "VERB \tverb \tis, say, told, given, playing, would\n",
    ". \tpunctuation marks \t. , ; !\n",
    "X \tother \tersatz, esprit, dunno, gr8, univeristy\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do more with books\n",
    "token_pos_tagged = nltk.pos_tag(text1.tokens)\n",
    "nouns = [x for x in token_pos_tagged if x[1] == \"NNP\"]\n",
    "#lesk(text1.tokens,'sacred', 'n') #.tree(lambda s:s.hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Dick\n",
      "[Synset('dick.n.01'), [Synset('detective.n.01'), [Synset('policeman.n.01'), [Synset('lawman.n.01'), [Synset('defender.n.01'), [Synset('preserver.n.03'), [Synset('person.n.01'), [Synset('causal_agent.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]], [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]]]\n",
      "[Lemma('dick.n.01.dick'), Lemma('dick.n.01.gumshoe'), Lemma('dick.n.01.hawkshaw')]\n",
      "someone who is a detective\n",
      "********************************************************************************\n",
      "Herman\n",
      "[Synset('herman.n.01')]\n",
      "[Lemma('herman.n.01.Herman'), Lemma('herman.n.01.Woody_Herman'), Lemma('herman.n.01.Woodrow_Charles_Herman')]\n",
      "United States jazz musician and bandleader (1913-1987)\n",
      "********************************************************************************\n",
      "Melville\n",
      "[Synset('melville.n.01')]\n",
      "[Lemma('melville.n.01.Melville'), Lemma('melville.n.01.Herman_Melville')]\n",
      "United States writer of novels and short stories (1819-1891)\n",
      "********************************************************************************\n",
      "]\n",
      "None\n",
      "********************************************************************************\n",
      "ETYMOLOGY\n",
      "[Synset('etymology.n.02'), [Synset('linguistics.n.01'), [Synset('science.n.01'), [Synset('discipline.n.01'), [Synset('knowledge_domain.n.01'), [Synset('content.n.05'), [Synset('cognition.n.01'), [Synset('psychological_feature.n.01'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]]]]]\n",
      "[Lemma('etymology.n.02.etymology')]\n",
      "the study of the sources and development of words\n",
      "********************************************************************************\n",
      "Consumptive\n",
      "[Synset('consumptive.n.01'), [Synset('sick_person.n.01'), [Synset('unfortunate.n.01'), [Synset('person.n.01'), [Synset('causal_agent.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]], [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]\n",
      "[Lemma('consumptive.n.01.consumptive'), Lemma('consumptive.n.01.lunger'), Lemma('consumptive.n.01.tubercular')]\n",
      "a person with pulmonary tuberculosis\n",
      "********************************************************************************\n",
      "Usher\n",
      "[Synset('usher.n.02'), [Synset('official.n.01'), [Synset('skilled_worker.n.01'), [Synset('worker.n.01'), [Synset('person.n.01'), [Synset('causal_agent.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]], [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]\n",
      "[Lemma('usher.n.02.usher'), Lemma('usher.n.02.doorkeeper')]\n",
      "an official stationed at the entrance of a courtroom or legislative chamber\n",
      "********************************************************************************\n",
      "Grammar\n",
      "[Synset('grammar.n.01'), [Synset('descriptive_linguistics.n.01'), [Synset('linguistics.n.01'), [Synset('science.n.01'), [Synset('discipline.n.01'), [Synset('knowledge_domain.n.01'), [Synset('content.n.05'), [Synset('cognition.n.01'), [Synset('psychological_feature.n.01'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]]]]]]\n",
      "[Lemma('grammar.n.01.grammar')]\n",
      "the branch of linguistics that deals with syntax and morphology (and sometimes also deals with semantics)\n",
      "********************************************************************************\n",
      "School\n",
      "[Synset('school.n.04'), [Synset('body.n.02'), [Synset('social_group.n.01'), [Synset('group.n.01'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]\n",
      "[Lemma('school.n.04.school')]\n",
      "a body of creative artists or writers or thinkers linked by a similar style or by similar teachers\n",
      "********************************************************************************\n",
      "Usher\n",
      "[Synset('usher.n.02'), [Synset('official.n.01'), [Synset('skilled_worker.n.01'), [Synset('worker.n.01'), [Synset('person.n.01'), [Synset('causal_agent.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]], [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]\n",
      "[Lemma('usher.n.02.usher'), Lemma('usher.n.02.doorkeeper')]\n",
      "an official stationed at the entrance of a courtroom or legislative chamber\n",
      "********************************************************************************\n",
      "H\n",
      "[Synset('henry.n.01'), [Synset('inductance_unit.n.01'), [Synset('electromagnetic_unit.n.01'), [Synset('unit_of_measurement.n.01'), [Synset('definite_quantity.n.01'), [Synset('measure.n.02'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]]]\n",
      "[Lemma('henry.n.01.henry'), Lemma('henry.n.01.H')]\n",
      "a unit of inductance in which an induced electromotive force of one volt is produced when the current is varied at the rate of one ampere per second\n",
      "********************************************************************************\n",
      ".\"\n",
      "None\n",
      "********************************************************************************\n",
      "HACKLUYT\n",
      "None\n",
      "********************************************************************************\n",
      "\"\n",
      "None\n",
      "********************************************************************************\n",
      "WHALE\n",
      "[Synset('whale.n.02'), [Synset('cetacean.n.01'), [Synset('aquatic_mammal.n.01'), [Synset('placental.n.01'), [Synset('mammal.n.01'), [Synset('vertebrate.n.01'), [Synset('chordate.n.01'), [Synset('animal.n.01'), [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]]]]\n",
      "[Lemma('whale.n.02.whale')]\n",
      "any of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the head\n",
      "********************************************************************************\n",
      "Sw\n",
      "[Synset('southwest.n.01'), [Synset('compass_point.n.01'), [Synset('direction.n.02'), [Synset('position.n.07'), [Synset('relation.n.01'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]]\n",
      "[Lemma('southwest.n.01.southwest'), Lemma('southwest.n.01.sou'-west'), Lemma('southwest.n.01.southwestward'), Lemma('southwest.n.01.SW')]\n",
      "the compass point midway between south and west; at 225 degrees\n",
      "********************************************************************************\n",
      "Dan\n",
      "None\n",
      "********************************************************************************\n",
      "HVAL\n",
      "None\n",
      "********************************************************************************\n",
      "Dan\n",
      "None\n",
      "********************************************************************************\n",
      "HVALT\n",
      "None\n",
      "********************************************************************************\n",
      "WEBSTER\n",
      "[Synset('webster.n.02')]\n",
      "[Lemma('webster.n.02.Webster'), Lemma('webster.n.02.Daniel_Webster')]\n",
      "United States politician and orator (1782-1817)\n",
      "********************************************************************************\n",
      "S\n",
      "[Synset('randomness.n.01'), [Synset('physical_property.n.01'), [Synset('property.n.02'), [Synset('attribute.n.02'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]\n",
      "[Lemma('randomness.n.01.randomness'), Lemma('randomness.n.01.entropy'), Lemma('randomness.n.01.S')]\n",
      "(thermodynamics) a thermodynamic quantity representing the amount of energy in a system that is no longer available for doing mechanical work\n",
      "********************************************************************************\n",
      "DICTIONARY\n",
      "[Synset('dictionary.n.01'), [Synset('wordbook.n.01'), [Synset('reference_book.n.01'), [Synset('book.n.01'), [Synset('publication.n.01'), [Synset('work.n.02'), [Synset('product.n.02'), [Synset('creation.n.02'), [Synset('artifact.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]]]\n",
      "[Lemma('dictionary.n.01.dictionary'), Lemma('dictionary.n.01.lexicon')]\n",
      "a reference book containing an alphabetical list of words with information about them\n",
      "********************************************************************************\n",
      "\"\n",
      "None\n",
      "********************************************************************************\n",
      "WHALE\n",
      "[Synset('whale.n.02'), [Synset('cetacean.n.01'), [Synset('aquatic_mammal.n.01'), [Synset('placental.n.01'), [Synset('mammal.n.01'), [Synset('vertebrate.n.01'), [Synset('chordate.n.01'), [Synset('animal.n.01'), [Synset('organism.n.01'), [Synset('living_thing.n.01'), [Synset('whole.n.02'), [Synset('object.n.01'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]]]]]]]]]]]]]\n",
      "[Lemma('whale.n.02.whale')]\n",
      "any of the larger cetacean mammals having a streamlined body and breathing through a blowhole on the head\n",
      "********************************************************************************\n",
      "Dut\n",
      "None\n",
      "********************************************************************************\n",
      "Ger\n",
      "None\n",
      "********************************************************************************\n",
      "WALLEN\n",
      "None\n",
      "********************************************************************************\n",
      "A\n",
      "[Synset('deoxyadenosine_monophosphate.n.01'), [Synset('nucleotide.n.01'), [Synset('ester.n.01'), [Synset('organic_compound.n.01'), [Synset('compound.n.02'), [Synset('chemical.n.01'), [Synset('material.n.01'), [Synset('substance.n.01'), [Synset('matter.n.03'), [Synset('physical_entity.n.01'), [Synset('entity.n.01')]]], [Synset('part.n.01'), [Synset('relation.n.01'), [Synset('abstraction.n.06'), [Synset('entity.n.01')]]]]]]]]]]]]\n",
      "[Lemma('deoxyadenosine_monophosphate.n.01.deoxyadenosine_monophosphate'), Lemma('deoxyadenosine_monophosphate.n.01.A')]\n",
      "one of the four nucleotides used in building DNA; all four nucleotides have a common phosphate group and a sugar (ribose)\n"
     ]
    }
   ],
   "source": [
    "# I am just printing 30 nouns tree if available for demonstration\n",
    "for value in nouns[1:30]:\n",
    "    print (\"*\"*80)\n",
    "    print (value[0])\n",
    "    #print (TagVsWordNetMap.get(value[1]))\n",
    "    if (lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])) is not None) :\n",
    "        print(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])).tree(lambda s:s.hypernyms()))\n",
    "        print(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])).lemmas())\n",
    "        #for ss in wn.synsets(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])).lemmas()):\n",
    "        #    print(ss, ss.definition())\n",
    "        print(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])).definition())\n",
    "    else:\n",
    "        print(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('alfred.n.01')\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[Synset('king.n.01'), Synset('negotiator.n.01'), Synset('living_thing.n.01'), Synset('person.n.01'), Synset('ruler.n.01'), Synset('representative.n.01'), Synset('organism.n.01'), Synset('entity.n.01'), Synset('head_of_state.n.01'), Synset('object.n.01'), Synset('sovereign.n.01'), Synset('physical_entity.n.01'), Synset('causal_agent.n.01'), Synset('whole.n.02'), Synset('communicator.n.01')]\n",
      "('king of Wessex; defeated the Vikings and encouraged writing in English '\n",
      " '(849-899)')\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "\"Alfred is :[Synset('king.n.01')]\"\n",
      "B is Synset('king.n.01')\n",
      "'This are kings as per hyponyms:'\n",
      "2.9444389791664407\n",
      "\"Similarity b/w :Synset('alfred.n.01') and Synset('ahab.n.01') is :\"\n",
      "3.6375861597263857\n",
      "['Alfred', 'Alfred_the_Great']\n",
      "['Ahab']\n",
      "['Akhenaton', 'Akhenaten', 'Ikhanaton', 'Amenhotep_IV']\n",
      "['Alaric']\n",
      "['Alfred', 'Alfred_the_Great']\n",
      "['Artaxerxes_I', 'Artaxerxes']\n",
      "[Lemma('alfred.n.01.Alfred'), Lemma('alfred.n.01.Alfred_the_Great')]\n",
      "'noun.person'\n",
      "[Synset('king.n.01')]\n",
      "13\n",
      "7\n",
      "[]\n",
      "[]\n",
      "'alfred.n.01'\n",
      "10813374\n",
      "[]\n",
      "[]\n",
      "[Synset('playing_card.n.01'), Synset('watch.n.01')]\n",
      "[Synset('sovereign.n.01')]\n",
      "[Synset('head_of_state.n.01'), Synset('ruler.n.01')]\n",
      "[Synset('person.n.01')]\n",
      "[Synset('causal_agent.n.01'), Synset('organism.n.01')]\n",
      "[Synset('living_thing.n.01')]\n",
      "[Synset('whole.n.02')]\n",
      "[]\n",
      "1.0\n",
      "0.5\n",
      "0.25\n",
      "0.16666666666666666\n",
      "'n'\n",
      "[]\n",
      "[Synset('entity.n.01')]\n",
      "1\n",
      "2\n",
      "3\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[Synset('king.n.01'),\n",
      " [Synset('king_of_england.n.01')],\n",
      " [Synset('king_of_france.n.01')],\n",
      " [Synset('king_of_the_germans.n.01')]]\n",
      "\"Synset('alfred.n.01')\"\n",
      "[]\n",
      "[]\n",
      "0.8148148148148148\n",
      "0.8461538461538461\n",
      "0.88\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "a = wn.lemma('dick.n.01.gumshoe').synset()\n",
    "a = wn.synset('bank.n.01')\n",
    "a = lesk(text1.tokens, 'Alfred', 'n')\n",
    "pprint (a)\n",
    "pprint (a.entailments())\n",
    "pprint (a.also_sees())\n",
    "pprint (a.causes())\n",
    "\n",
    "print (a.common_hypernyms(wn.synset('king.n.01')))\n",
    "pprint (a.definition())\n",
    "pprint (a.entailments())\n",
    "pprint (a.examples())\n",
    "pprint (a.frame_ids())\n",
    "#pprint (a.hypernym_distances())\n",
    "#pprint (a.hypernym_paths())\n",
    "pprint (a.hypernyms())\n",
    "pprint (a.hyponyms())\n",
    "pprint (\"Alfred is :{}\".format(a.instance_hypernyms())) # Very good to predict who is Alfred(our input synset)\n",
    "b = a.instance_hypernyms()\n",
    "print (\"B is {}\".format(b[0]))\n",
    "pprint (\"This are kings as per hyponyms:\")\n",
    "#pprint (b[0].instance_hyponyms()) # Very good to know descendants : example King can have \"Ashoka\", \"ChandraGupta\" as leaf nodes\n",
    "#help(a.jcn_similarity)\n",
    "#pprint (a.jcn_similarity(b[0])) # to learn \n",
    "\n",
    "pprint (a.lch_similarity(b[0]))\n",
    "pprint (\"Similarity b/w :{} and {} is :\".format(a,b[0].instance_hyponyms()[0]))\n",
    "pprint (a.lch_similarity(a,b[0].instance_hyponyms()[0]))\n",
    "\n",
    "pprint (a.lemma_names()) \n",
    "for i in b[0].instance_hyponyms()[0:5]: # Print few lemma names who are hyponyms of King synset\n",
    "    pprint (i.lemma_names()) # Print one of the hyponym of King synset\n",
    "\n",
    "pprint (a.lemmas()) # Print all lemmas without names; Lemmas are in format <synset>.<lemma_name>; there can be multiple names for single synset, so multiple lemmas would be there.\n",
    "\n",
    "pprint (a.lexname()) # Lexname would print \"noun/person\" if name is passed\n",
    "\n",
    "pprint (b[0].instance_hyponyms()[0].lowest_common_hypernyms(b[0].instance_hyponyms()[1])) # It gives immediate hypernym, \n",
    "# pass two king names it would give king-synset back\n",
    "\n",
    "pprint(a.max_depth())\n",
    "pprint(a.min_depth())\n",
    "\n",
    "pprint(a.member_holonyms())\n",
    "\n",
    "pprint(a.member_meronyms())\n",
    "\n",
    "pprint(a.name())\n",
    "\n",
    "pprint(a.offset())\n",
    "#help(wordnet.all_lemma_names)\n",
    "d = wordnet.all_lemma_names(pos='n',lang='eng')\n",
    "\n",
    "pprint(a.part_holonyms())\n",
    "pprint(b[0].part_holonyms())\n",
    "print(lesk(text1.tokens, 'face', 'n').part_holonyms())\n",
    "\n",
    "pprint(b[0].hypernyms()) # One above king\n",
    "pprint(b[0].hypernyms()[0].hypernyms()) # Two level above king\n",
    "pprint(b[0].hypernyms()[0].hypernyms()[1].hypernyms()) # 3 level above king\n",
    "pprint(b[0].hypernyms()[0].hypernyms()[1].hypernyms()[0].hypernyms())\n",
    "pprint(b[0].hypernyms()[0].hypernyms()[1].hypernyms()[0].hypernyms()[1].hypernyms())\n",
    "pprint(b[0].hypernyms()[0].hypernyms()[1].hypernyms()[0].hypernyms()[1].hypernyms()[0].hypernyms())\n",
    "\n",
    "pprint(a.part_meronyms())\n",
    "\n",
    "pprint(a.path_similarity(a)) # this return 1: since path from top to bottom is same\n",
    "pprint(a.path_similarity(b[0])) # this returns .5: since path from top to bottom is common, but not same\n",
    "pprint(a.path_similarity(b[0].hypernyms()[0].hypernyms()[1]))\n",
    "pprint(a.path_similarity(b[0].hypernyms()[0].hypernyms()[1].hypernyms()[0].hypernyms()[1]))\n",
    "# Total different items would have very less path_similarity\n",
    "\n",
    "pprint(a.pos()) # prints 'n'; since it is noun\n",
    "\n",
    "pprint(a.region_domains())\n",
    "\n",
    "#pprint(a.res_similarity(b[0]))  #TBD\n",
    "\n",
    "pprint(a.root_hypernyms()) # prints root node \"entity in our case\n",
    "\n",
    "pprint(a.shortest_path_distance(b[0])) # prints 1; since alfred is one away from king\n",
    "pprint(a.shortest_path_distance(b[0].hypernyms()[0])) # prints 2 : since alfred is two node away from sovereign\n",
    "pprint(a.shortest_path_distance(b[0].hypernyms()[0].hypernyms()[1])) # prints 3\n",
    "\n",
    "pprint(a.similar_tos())\n",
    "pprint(b[0].hypernyms()[0].hypernyms()[1].similar_tos())\n",
    "\n",
    "pprint(a.substance_holonyms())\n",
    "pprint(b[0].hypernyms()[0].substance_holonyms())\n",
    "pprint(a.substance_meronyms())\n",
    "\n",
    "pprint(b[0].hypernyms()[0].topic_domains())\n",
    "\n",
    "pprint(b[0].tree(lambda x: x.hyponyms())) # Print tree from king to down; upper nodes not printed in this, upper nodes can be printed using hyperonyms()\n",
    "\n",
    "pprint(a.unicode_repr())\n",
    "\n",
    "pprint(b[0].usage_domains())\n",
    "\n",
    "pprint(b[0].verb_groups())\n",
    "\n",
    "pprint(a.wup_similarity(b[0]))\n",
    "pprint(a.wup_similarity(b[0].hypernyms()[0]))\n",
    "pprint(a.wup_similarity(b[0].hypernyms()[0].hypernyms()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260819\n",
      "king of Wessex; defeated the Vikings and encouraged writing in English (849-899)\n",
      "Synset('alfred.n.01')\n",
      "a small portable timepiece\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print (len(text1))\n",
    "print (lesk(text1.tokens, 'Alfred', 'n').definition())\n",
    "print(lesk(text1.tokens, 'Alfred', 'n'))\n",
    "\n",
    "print(lesk(text1.tokens, 'face', 'n').part_holonyms()[1].definition())\n",
    "print(lesk(text1.tokens, 'parent', 'n').member_meronyms())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Temples'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatizer.lemmatize(\"Temples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "filesDir = \"/home/tivo/speeches\"\n",
    "\n",
    "newcorpus = PlaintextCorpusReader(filesDir, '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[[Synset('entity.n.01'), Synset('abstraction.n.06'), Synset('psychological_feature.n.01'), Synset('cognition.n.01'), Synset('content.n.05'), Synset('belief.n.01'), Synset('doctrine.n.01'), Synset('theological_doctrine.n.01'), Synset('predestination.n.02'), Synset('election.n.04')]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'hypernym_paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-2da1cfc50b85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Election'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypernym_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlesk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Barack'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypernym_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#print (c)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'hypernym_paths'"
     ]
    }
   ],
   "source": [
    "file = newcorpus.open('/home/tivo/speeches/obama/obama001.txt')\n",
    "a = file.readlines()\n",
    "print (type(a))\n",
    "b = \" \".join(a)\n",
    "c = nltk.pos_tag(tokenizer.tokenize(b))\n",
    "\n",
    "print (lesk(c,'Election','n').hypernym_paths())\n",
    "print (lesk(c,'Barack','n').hypernym_paths())\n",
    "#print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few things pending above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import rdflib\n",
    "graph = rdflib.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph.parse('http://dbpedia.org/resource/Semantic_Web')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(graph)\n",
    "#graph.parse('http://dbpedia.org/page/Barack_Obama')\n",
    "#help(rdflib.Namespace)\n",
    "#namespace = rdflib.Namespace('http://dbpedia.org/page/Barack_Obama')\n",
    "#print(namespace)\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?p ?s $o\n",
    "    WHERE { <http://dbpedia.org/page/Barack_Obama> rdfs:label ?label }\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "print (results)\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(result[\"label\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.parse(\"http://dbpedia.org/resource/Barack_Obama\")\n",
    "\n",
    "print (len(g))\n",
    "\n",
    "# To print all\n",
    "#print (list(g))\n",
    "\n",
    "for stmt in g.subject_objects(URIRef(\"http://dbpedia.org/ontology/birthDate\")):\n",
    "     print (stmt)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stmt in g.subject_objects(URIRef(\"http://dbpedia.org/ontology/birthDate\")):\n",
    "    print (\"the person represented by\", str(stmt[0]), \"was born on\", str(stmt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for stmt in g.subject_objects(URIRef(\"http://dbpedia.org/ontology/spouse\")):\n",
    "    print (\"the person represented by\", str(stmt[0]), \"was married to\", str(stmt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "import sys\n",
    "import urllib\n",
    "\n",
    "def extract_words(text):\n",
    "    '''\n",
    "    here we are extracting features to use in our classifier. We want to pull all the words in our input\n",
    "    porterstem them and grab the most significant bigrams to add to the mix as well.\n",
    "    '''\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "\n",
    "    for bigram_tuple in bigrams:\n",
    "        x = \"%s %s\" % bigram_tuple\n",
    "        tokens.append(x)\n",
    "    #print (tokens)\n",
    "    result =  [stemmer.stem(x.lower()) for x in tokens if x not in stopwords.words('english') and len(x) > 1]\n",
    "    return result\n",
    "\n",
    "def get_feature(word):\n",
    "    return dict([(word, True)])\n",
    "\n",
    "\n",
    "def bag_of_words(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "\n",
    "def create_training_dict(text, sense):\n",
    "    ''' returns a dict ready for a classifier's test method '''\n",
    "    tokens = extract_words(text)\n",
    "    return [(bag_of_words(tokens), sense)]\n",
    "\n",
    "\n",
    "\n",
    "def run_classifier_tests(classifier):\n",
    "    testfiles = [{'fruit': 'http://litfuel.net/plush/files/disambiguation/apple-fruit-training.txt'},\n",
    "                 {'company': 'http://litfuel.net/plush/files/disambiguation/apple-company-training.txt'}]\n",
    "    testfeats = []\n",
    "    for file in testfiles:\n",
    "        for sense, loc in file.iteritems():\n",
    "            for line in urllib.request.build_opener(loc):\n",
    "                testfeats = testfeats + create_training_dict(line, sense)\n",
    "\n",
    "\n",
    "    acc = accuracy(classifier, testfeats) * 100\n",
    "    print ('accuracy: %.2f%%' % acc)\n",
    "\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "# create our dict of training data\n",
    "texts = {}\n",
    "texts['fruit'] = 'http://litfuel.net/plush/files/disambiguation/apple-fruit.txt'\n",
    "texts['company'] = 'http://litfuel.net/plush/files/disambiguation/apple-company.txt'\n",
    "\n",
    "#holds a dict of features for training our classifier\n",
    "train_set = []\n",
    "\n",
    "# loop through each item, grab the text, tokenize it and create a training feature with it\n",
    "for sense, file in texts.items():\n",
    "    print (\"training %s \" % sense)\n",
    "    #print (type(file))\n",
    "    #b = bytearray()\n",
    "    #b.extend(map(ord, file))\n",
    "    #text = urllib.request.http(b, 'r').read()\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "    #print (opener.open(url).read())\n",
    "    #json.loads(opener.open(file).read().decode('utf-8'))\n",
    "    features = extract_words(opener.open(file).read().decode())\n",
    "    train_set = train_set + [(get_feature(word), sense) for word in features]\n",
    "\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# uncomment out this line to see the most informative words the classifier will use\n",
    "#classifier.show_most_informative_features(20)\n",
    "\n",
    "\n",
    "# uncomment out this line to see how well our accuracy is using some hand curated tweets\n",
    "#run_classifier_tests(classifier)\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "#print(opener.open(file).read().decode())\n",
    "\n",
    "for line in opener.open(file).readLines():\n",
    "    tokens = bag_of_words(extract_words(line))\n",
    "    #print (line)\n",
    "    decision = classifier.classify(tokens)\n",
    "    result = \"%s - %s\" % (decision,line )\n",
    "    #print (result)\n",
    "#print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salman Khan\n"
     ]
    }
   ],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT ?label\n",
    "    WHERE { \n",
    "      <http://dbpedia.org/resource/Salman_Khan> rdfs:label ?label .\n",
    "    }\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "#print (results)\n",
    "a = results[\"results\"][\"bindings\"]\n",
    "#a = {'label':'value'}\n",
    "#print (a.get('label'))\n",
    "#print (a)\n",
    "def getEnglish(a,key):\n",
    "    for x in a:\n",
    "        label = x.get(key)\n",
    "        if (label.get('xml:lang') == 'en'):\n",
    "            return label\n",
    "        \n",
    "print (getEnglish(a,'label')['value'])\n",
    "#for result in results[\"results\"][\"bindings\"]:\n",
    "#    print (result[\"label\"][\"value\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX quepy: <http://www.machinalis.com/quepy#>\n",
    "    PREFIX dbpedia: <http://dbpedia.org/ontology/>\n",
    "    PREFIX dbpprop: <http://dbpedia.org/property/>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "\n",
    "    SELECT DISTINCT ?x1 WHERE {\n",
    "      ?x0 rdf:type foaf:Person.\n",
    "      ?x0 rdfs:label \"Salman Khan\"@en.\n",
    "      ?x0 rdfs:comment ?x1.\n",
    "    }\n",
    "    \"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "#print (results)\n",
    "textFromDbPedia = getEnglish(results[\"results\"][\"bindings\"],'x1')['value']\n",
    "#for result in results[\"results\"][\"bindings\"]:\n",
    "#    print (result[\"x1\"][\"value\"])\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('/home/tivo/tivo/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "'/home/tivo/tivo/stanford-ner/stanford-ner.jar')\n",
    "a = st.tag(getEnglish(results[\"results\"][\"bindings\"],'x1')['value'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#textFromDbPedia\n",
    "#a\n",
    "textFromDbPedia_posTagged = nltk.pos_tag(textFromDbPedia.split())\n",
    "textFromDbPedia_posTagged_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (textFromDbPedia_posTagged)\n",
    "print (\"*\"*80)\n",
    "#print (a)\n",
    "\n",
    "def SentenceLesk(line):\n",
    "    a= nltk.pos_tag(line.split())\n",
    "    for value in a:\n",
    "        print (\"*\"*80)\n",
    "        print (value[0])\n",
    "        #print (TagVsWordNetMap.get(value[1]))\n",
    "        lesk_local = lesk(line,value[0], TagVsWordNetMap.get(value[1]))\n",
    "        if (lesk_local is not None) :\n",
    "            print(lesk_local.tree(lambda s:s.hypernyms()))\n",
    "            print(lesk_local.lemmas())\n",
    "            print(lesk_local.definition())\n",
    "            #for ss in wn.synsets(lesk(text1.tokens,value[0], TagVsWordNetMap.get(value[1])).lemmas()):\n",
    "            #    print(ss, ss.definition())\n",
    "            for i in lesk_local.lemmas():\n",
    "                print (i)\n",
    "                #print(i.definition())\n",
    "        else:\n",
    "            print(lesk_local)\n",
    "\n",
    "    \n",
    "\n",
    "textFromDbPedia_posTagged_nouns = [x[0] for x in textFromDbPedia_posTagged if x[1] == \"NNP\"]\n",
    "st.tag(textFromDbPedia_posTagged_nouns)\n",
    "#print (lesk(textFromDbPedia,'Salman','n'))\n",
    "#print (lesk(\"I went to Abraham Lincoln\".split(),'Abraham','n'))\n",
    "\n",
    "SentenceLesk(\"I went to George Cloney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "REST_URL = \"http://data.bioontology.org\"\n",
    "API_KEY = \"4bc38ba7-1768-4ddb-8989-cf302a094d97\"\n",
    "\n",
    "def get_json(url):\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('Authorization', 'apikey token=' + API_KEY)]\n",
    "    #print (opener.open(url).read())\n",
    "    return json.loads(opener.open(url).read().decode('utf-8'))\n",
    "\n",
    "def print_annotations(annotations, get_class=True):\n",
    "    for result in annotations:\n",
    "        class_details = get_json(result[\"annotatedClass\"][\"links\"][\"self\"]) if get_class else result[\"annotatedClass\"]\n",
    "        print (\"Class details\")\n",
    "        print (\"\\tid: \" + class_details[\"@id\"])\n",
    "        print (\"\\tprefLabel: \" + class_details[\"prefLabel\"])\n",
    "        print (\"\\tontology: \" + class_details[\"links\"][\"ontology\"])\n",
    "\n",
    "        print (\"Annotation details\")\n",
    "        for annotation in result[\"annotations\"]:\n",
    "            print (\"\\tfrom: \" + str(annotation[\"from\"]))\n",
    "            print (\"\\tto: \" + str(annotation[\"to\"]))\n",
    "            print (\"\\tmatch type: \" + annotation[\"matchType\"])\n",
    "\n",
    "        if result[\"hierarchy\"]:\n",
    "            print (\"\\n\\tHierarchy annotations\")\n",
    "            for annotation in result[\"hierarchy\"]:\n",
    "                class_details = get_json(annotation[\"annotatedClass\"][\"links\"][\"self\"])\n",
    "                pref_label = class_details[\"prefLabel\"] or \"no label\"\n",
    "                print (\"\\t\\tClass details\")\n",
    "                print (\"\\t\\t\\tid: \" + class_details[\"@id\"])\n",
    "                print (\"\\t\\t\\tprefLabel: \" + class_details[\"prefLabel\"])\n",
    "                print (\"\\t\\t\\tontology: \" + class_details[\"links\"][\"ontology\"])\n",
    "                print (\"\\t\\t\\tdistance from originally annotated class: \" + str(annotation[\"distance\"]))\n",
    "\n",
    "        print (\"\\n\\n\")\n",
    "\n",
    "text_to_annotate = \"Melanoma is a malignant tumor of melanocytes which are found predominantly in skin but also in the bowel and the eye.\"\n",
    "\n",
    "# Annotate using the provided text\n",
    "print (str(str(REST_URL) + str(\"/annotator?text=\") + str(urllib.parse.quote(text_to_annotate))))\n",
    "annotations = get_json(str(str(REST_URL) + str(\"/annotator?text=\") + str(urllib.parse.quote(text_to_annotate))))\n",
    "\n",
    "# Print out annotation details\n",
    "#print_annotations(annotations)\n",
    "\n",
    "# Annotate with hierarchy information\n",
    "annotations = get_json(REST_URL + \"/annotator?max_level=3&text=\" + urllib.parse.quote(text_to_annotate))\n",
    "#print_annotations(annotations)\n",
    "\n",
    "# Annotate with prefLabel, synonym, definition returned\n",
    "annotations = get_json(REST_URL + \"/annotator?include=prefLabel,synonym,definition&text=\" + urllib.parse.quote(text_to_annotate))\n",
    "#print_annotations(annotations, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(graph)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s, p, o in list(graph)[:10]:\n",
    "    print (s, \"p--- \", p, \"o------ \", o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rdflib import ConjunctiveGraph, Namespace, exceptions\n",
    "\n",
    "from rdflib import URIRef, RDFS, RDF, BNode\n",
    "\n",
    "import owl as OWL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class OntoInspector(object):\n",
    "\n",
    "    \"\"\"Class that includes methods for querying an RDFS/OWL ontology\"\"\"        \n",
    "\n",
    "    def __init__(self, uri, language=\"\"):\n",
    "        super(OntoInspector, self).__init__()\n",
    "\n",
    "        self.rdfGraph = ConjunctiveGraph()\n",
    "        try:\n",
    "            self.rdfGraph.parse(uri, format=\"xml\")\n",
    "        except:\n",
    "            try:\n",
    "                self.rdfGraph.parse(uri, format=\"n3\")\n",
    "            except:\n",
    "                raise exceptions.Error(\"Could not parse the file! Is it a valid RDF/OWL ontology?\")\n",
    "\n",
    "        finally:\n",
    "            # let's cache some useful info for faster access\n",
    "            self.baseURI = self.get_OntologyURI() or uri            \n",
    "            self.allclasses = self.__getAllClasses(classPredicate)\n",
    "            self.toplayer = self.__getTopclasses()\n",
    "            self.tree = self.__getTree()\n",
    "\n",
    "\n",
    "    def get_OntologyURI(self, return_as_string=True):\n",
    "        \"\"\" \n",
    "        In [15]: [x for x in o.rdfGraph.triples((None, RDF.type, OWL.Ontology))]\n",
    "        Out[15]: \n",
    "        [(rdflib.URIRef('http://purl.com/net/sails'),\n",
    "          rdflib.URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),\n",
    "          rdflib.URIRef('http://www.w3.org/2002/07/owl#Ontology'))]\n",
    "\n",
    "        Mind that this will work only for OWL ontologies.\n",
    "        In other cases we just return None, and use the URI passed at loading time\n",
    "        \"\"\"\n",
    "\n",
    "        test = [x for x, y, z in self.rdfGraph.triples((None, RDF.type, OWL.Ontology))]\n",
    "\n",
    "        if test:\n",
    "            if return_as_string:\n",
    "                return str(test[0])\n",
    "            else:\n",
    "                return test[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def __getAllClasses(self, classPredicate = \"\", removeBlankNodes = True):\n",
    "        \"\"\"  \n",
    "        Extracts all the classes from a model\n",
    "        We use the RDFS and OWL predicate by default; also, we extract non explicitly declared classes\n",
    "        \"\"\"\n",
    "\n",
    "        rdfGraph = self.rdfGraph\n",
    "        exit = []       \n",
    "\n",
    "        if not classPredicate:          \n",
    "            for s, v, o in rdfGraph.triples((None, RDF.type , OWL.Class)): \n",
    "                exit.append(s)\n",
    "            for s, v, o in rdfGraph.triples((None, RDF.type , RDFS.Class)):\n",
    "                exit.append(s)\n",
    "\n",
    "            # this extra routine makes sure we include classes not declared explicitly\n",
    "            # eg when importing another onto and subclassing one of its classes...\n",
    "            for s, v, o in rdfGraph.triples((None, RDFS.subClassOf , None)):\n",
    "                if s not in exit:\n",
    "                    exit.append(s)\n",
    "                if o not in exit:\n",
    "                    exit.append(o)\n",
    "\n",
    "            # this extra routine includes classes found only in rdfs:domain and rdfs:range definitions\n",
    "            for s, v, o in rdfGraph.triples((None, RDFS.domain , None)):\n",
    "                if o not in exit:\n",
    "                    exit.append(o)\n",
    "            for s, v, o in rdfGraph.triples((None, RDFS.range , None)):\n",
    "                if o not in exit:\n",
    "                    exit.append(o)\n",
    "\n",
    "        else:\n",
    "            if classPredicate == \"rdfs\" or classPredicate == \"rdf\":\n",
    "                for s, v, o in rdfGraph.triples((None, RDF.type , RDFS.Class)):\n",
    "                    exit.append(s)\n",
    "            elif classPredicate == \"owl\":\n",
    "                for s, v, o in rdfGraph.triples((None, RDF.type , OWL.Class)): \n",
    "                    exit.append(s)\n",
    "            else:\n",
    "                raise exceptions.Error(\"ClassPredicate must be either rdf, rdfs or owl\")\n",
    "\n",
    "        exit = remove_duplicates(exit)\n",
    "\n",
    "        if removeBlankNodes:\n",
    "            exit = [x for x in exit if not self.__isBlankNode(x)]\n",
    "\n",
    "        return sort_uri_list_by_name(exit)\n",
    "\n",
    "    # methods for getting ancestores and descendants of classes: by default, we do not include blank nodes\n",
    "\n",
    "    def get_classDirectSupers(self, aClass, excludeBnodes = True):\n",
    "        returnlist = []\n",
    "        for s, v, o in self.rdfGraph.triples((aClass, RDFS.subClassOf , None)):\n",
    "            if excludeBnodes:\n",
    "                if not self.__isBlankNode(o):\n",
    "                    returnlist.append(o)\n",
    "            else:\n",
    "                returnlist.append(o)\n",
    "\n",
    "        return sort_uri_list_by_name(remove_duplicates(returnlist)) \n",
    "\n",
    "\n",
    "    def get_classDirectSubs(self, aClass, excludeBnodes = True):\n",
    "        returnlist = []\n",
    "        for s, v, o in self.rdfGraph.triples((None, RDFS.subClassOf , aClass)):\n",
    "            if excludeBnodes:\n",
    "                if not self.__isBlankNode(s):\n",
    "                    returnlist.append(s)\n",
    "\n",
    "            else:\n",
    "                returnlist.append(s)\n",
    "\n",
    "        return sort_uri_list_by_name(remove_duplicates(returnlist))\n",
    "\n",
    "\n",
    "    def get_classAllSubs(self, aClass, returnlist = [], excludeBnodes = True):\n",
    "        for sub in self.get_classDirectSubs(aClass, excludeBnodes):\n",
    "            returnlist.append(sub)\n",
    "            self.get_classAllSubs(sub, returnlist, excludeBnodes)\n",
    "        return sort_uri_list_by_name(remove_duplicates(returnlist))\n",
    "\n",
    "\n",
    "\n",
    "    def get_classAllSupers(self, aClass, returnlist = [], excludeBnodes = True ):\n",
    "        for ssuper in self.get_classDirectSupers(aClass, excludeBnodes):\n",
    "            returnlist.append(ssuper)\n",
    "            self.get_classAllSupers(ssuper, returnlist, excludeBnodes)\n",
    "        return sort_uri_list_by_name(remove_duplicates(returnlist))\n",
    "\n",
    "\n",
    "\n",
    "    def get_classSiblings(self, aClass, excludeBnodes = True):\n",
    "        returnlist = []\n",
    "        for father in self.get_classDirectSupers(aClass, excludeBnodes):\n",
    "            for child in self.get_classDirectSubs(father, excludeBnodes):\n",
    "                if child != aClass:\n",
    "                    returnlist.append(child)\n",
    "\n",
    "        return sort_uri_list_by_name(remove_duplicates(returnlist))\n",
    "\n",
    "\n",
    "    def __getTopclasses(self, classPredicate = ''):\n",
    "\n",
    "        \"\"\" Finds the topclass in an ontology (works also when we have more than on superclass)\n",
    "        \"\"\"\n",
    "\n",
    "        returnlist = []\n",
    "\n",
    "        # gets all the classes\n",
    "        for eachclass in self.__getAllClasses(classPredicate):\n",
    "            x = self.get_classDirectSupers(eachclass)\n",
    "            if not x:\n",
    "                returnlist.append(eachclass)\n",
    "\n",
    "        return sort_uri_list_by_name(returnlist)\n",
    "\n",
    "    def __getTree(self, father=None, out=None):\n",
    "\n",
    "        \"\"\" Reconstructs the taxonomical tree of an ontology, from the 'topClasses' (= classes with no supers, see below)\n",
    "            Returns a dictionary in which each class is a key, and its direct subs are the values.\n",
    "            The top classes have key = 0\n",
    "\n",
    "            Eg.\n",
    "            {'0' : [class1, class2], class1: [class1-2, class1-3], class2: [class2-1, class2-2]}\n",
    "        \"\"\"\n",
    "\n",
    "        if not father:\n",
    "            out = {}\n",
    "            topclasses = self.toplayer\n",
    "            out[0] = topclasses\n",
    "\n",
    "            for top in topclasses:\n",
    "                children = self.get_classDirectSubs(top)\n",
    "                out[top] = children\n",
    "                for potentialfather in children:\n",
    "                    self.__getTree(potentialfather, out)\n",
    "\n",
    "            return out\n",
    "\n",
    "        else:\n",
    "            children = self.get_classDirectSubs(father)\n",
    "            out[father] = children\n",
    "            for ch in children:\n",
    "                self.__getTree(ch, out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from onto_inspector import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from onto_inspector import *\n",
    "\n",
    "onto = OntoInspector(\"http://xmlns.com/foaf/spec/20100809.rdf\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onto.toplayer\n",
    "\n",
    "\n",
    "onto.printTree()\n",
    "\n",
    "\n",
    "document = onto.find_class_byname(\"document\")\n",
    "\n",
    "document\n",
    "\n",
    "document = document[0]\n",
    "\n",
    "document\n",
    "\n",
    "onto.get_classAllSubs(document)\n",
    "\n",
    "\n",
    "onto.get_classAllSupers(document)\n",
    "\n",
    "onto.get_classComment(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "QueryBadFormed",
     "evalue": "QueryBadFormed: a bad request has been sent to the endpoint, probably the sparql query is bad formed. \n\nResponse:\nb\"Virtuoso 37000 Error SP030: SPARQL compiler, line 19: syntax error at 'Sallu'\\n\\nSPARQL query:\\ndefine sql:big-data-const 0 \\n#output-format:application/sparql-results+json\\n\\n    PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\\n    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\\n    PREFIX quepy: <http://www.machinalis.com/quepy#>\\n    PREFIX dbpedia: <http://dbpedia.org/ontology/>\\n    PREFIX dbpprop: <http://dbpedia.org/property/>\\n    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\\n\\n    SELECT DISTINCT ?x1 WHERE {\\n      ?x0 rdf:type foaf:Person.\\n      ?x0 rdfs:label ?x2.\\n      ?x0 rdfs:abstract ?s1.\\n      ?x0 rdfs:comment ?x1.\\n      FILTER(contains(?s1,Sallu))\\n    }\\n    \"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturnFormat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    581\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 582\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mQueryBadFormed\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-1ebe42b935d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \"\"\")\n\u001b[0;32m     57\u001b[0m \u001b[0msparql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetReturnFormat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJSON\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mtextFromDbPedia\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetEnglish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"results\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bindings\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'x1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36mquery\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m             \u001b[1;33m@\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mQueryResult\u001b[0m\u001b[1;33m}\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m--> 567\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mQueryResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mqueryAndConvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tivo/anaconda3/lib/python3.5/site-packages/SPARQLWrapper/Wrapper.py\u001b[0m in \u001b[0;36m_query\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryBadFormed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mEndPointNotFound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mQueryBadFormed\u001b[0m: QueryBadFormed: a bad request has been sent to the endpoint, probably the sparql query is bad formed. \n\nResponse:\nb\"Virtuoso 37000 Error SP030: SPARQL compiler, line 19: syntax error at 'Sallu'\\n\\nSPARQL query:\\ndefine sql:big-data-const 0 \\n#output-format:application/sparql-results+json\\n\\n    PREFIX owl: <http://www.w3.org/2002/07/owl#>\\n    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\\n    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\\n    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\\n    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\\n    PREFIX quepy: <http://www.machinalis.com/quepy#>\\n    PREFIX dbpedia: <http://dbpedia.org/ontology/>\\n    PREFIX dbpprop: <http://dbpedia.org/property/>\\n    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\\n\\n    SELECT DISTINCT ?x1 WHERE {\\n      ?x0 rdf:type foaf:Person.\\n      ?x0 rdfs:label ?x2.\\n      ?x0 rdfs:abstract ?s1.\\n      ?x0 rdfs:comment ?x1.\\n      FILTER(contains(?s1,Sallu))\\n    }\\n    \""
     ]
    }
   ],
   "source": [
    "def DBdisambiguation(name, sparql):\n",
    "    #query = \"select distinct ?page where { \\\n",
    "    #?syn (dbpedia-owl:wikiPageDisambiguates|^dbpedia-owl:wikiPageDisambiguates)* dbpedia:\"+name+\" ; \\\n",
    "    #foaf:isPrimaryTopicOf ?page \\\n",
    "    #}\"\n",
    "    \n",
    "    #query = \n",
    "    #print (query)\n",
    "    sparql.setQuery(\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX quepy: <http://www.machinalis.com/quepy#>\n",
    "    PREFIX dbpedia: <http://dbpedia.org/ontology/>\n",
    "    PREFIX dbpprop: <http://dbpedia.org/property/>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "    \n",
    "    SELECT DISTINCT ?syn WHERE { \n",
    "    { \n",
    "        ?disPage dbpedia-owl:wikiPageDisambiguates <http://dbpedia.org/resource/\"\"\"+name+\"\"\">. \n",
    "        ?disPage dbpedia-owl:wikiPageDisambiguates ?syn. \n",
    "    }\n",
    "    UNION \n",
    "    { \n",
    "        <http://dbpedia.org/resource/\"\"\"+name+\"\"\"> dbpedia-owl:wikiPageDisambiguates ?syn. \n",
    "    } \n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "    sparql.setReturnFormat(JSON)  \n",
    "    results_list = sparql.query().convert()\n",
    "    return results_list\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX quepy: <http://www.machinalis.com/quepy#>\n",
    "    PREFIX dbpedia: <http://dbpedia.org/ontology/>\n",
    "    PREFIX dbpprop: <http://dbpedia.org/property/>\n",
    "    PREFIX dbpedia-owl: <http://dbpedia.org/ontology/>\n",
    "\n",
    "    SELECT DISTINCT ?x1 WHERE {\n",
    "      ?x0 rdf:type foaf:Person.\n",
    "      ?x0 rdfs:label ?x2.\n",
    "      ?x0 rdfs:abstract ?s1.\n",
    "      ?x0 rdfs:comment ?x1.\n",
    "      FILTER(contains(?s1,Sallu))\n",
    "    }\n",
    "    \"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "print (results)\n",
    "textFromDbPedia = getEnglish(results[\"results\"][\"bindings\"],'x1')['value']\n",
    "#for result in results[\"results\"][\"bindings\"]:\n",
    "#    print (result[\"x1\"][\"value\"])\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "st = StanfordNERTagger('/home/tivo/tivo/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "'/home/tivo/tivo/stanford-ner/stanford-ner.jar')\n",
    "a = st.tag(getEnglish(results[\"results\"][\"bindings\"],'x1')['value'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'results': {'bindings': [{'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Ali_Khan,_Iran'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Ali_Khan_(activist)'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Ali_Khan_(brigadier)'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Nusrat_Fateh_Ali_Khan'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Liaquat_Ali_Khan'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Ali_Khan_(Sufi)'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_Ali_Khan'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Faisal_Saif'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_Ahmad_Al_Ghurair'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Said_Saif_Asaad'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Seyf'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_al-Islam_Gaddafi'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Scimitar'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/HL7_Services_Aware_Interoperability_Framework'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Mostafa_Elwi_Saif'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_Ahmad'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_Mohammed_Al_Bishr'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_Saaeed_Shaheen'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_al-Adel'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_al-Arab_Gaddafi'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_al-Din'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_bin_Sultan'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Saif_bin_Zayed_Al_Nahyan'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Salih_Saif_Aldin'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Shanghai_Advanced_Institute_of_Finance'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/State_Accident_Insurance_Fund'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Umar_Saif'}}, {'syn': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Yousef_Saif'}}], 'distinct': False, 'ordered': True}, 'head': {'link': [], 'vars': ['syn']}}\n"
     ]
    }
   ],
   "source": [
    "print (DBdisambiguation(\"Saif_Ali_Khan\", sparql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
